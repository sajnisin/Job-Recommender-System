# -*- coding: utf-8 -*-
"""Job Recommender_Final.ipynb

Automatically generated by Colab.

# CS5344 Group Project - Job Recommender System

**Project Group 14**

The codes were executed in Google Colab as a Python Notebook (.ipynb).

Key Environments:
- Python version 3.10.12
- OpenJDK version 11.0.22
- Pyspark version 3.5.1
- Scala version 2.12.18
- nltk version 3.8.1
- numpy version 1.25.2

The notebook was then exported to Job_Recommender_Spark_Codes.py

Command execution to run .py file:
!python Job_Recommender_Spark_Codes.py linkedin_job_posts_skills.csv stopwords.txt test_cases.csv output_job_skills_match.csv

## 1. Setup
"""

# !pip install --quiet pyspark   #  pyspark 3.5.1

import sys
import os
import json
import re
import string
import csv

import pyspark
from pyspark.sql import SparkSession, Row
from pyspark.sql.functions import udf, col, lower, desc, split, trim, expr
from pyspark.sql.types import DoubleType, StringType, BooleanType, ArrayType
from pyspark.ml.feature import Tokenizer, RegexTokenizer, StopWordsRemover, HashingTF, IDF, CountVectorizer
from pyspark.ml.linalg import Vectors, VectorUDT

import nltk
from nltk.tokenize import sent_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# For Google Colaboratory
# import sys, os
# if 'google.colab' in sys.modules:
#     # mount google drive
#     from google.colab import drive
#     drive.mount('/content/gdrive')
#     path_to_file = '/content/gdrive/MyDrive/CS5344'    # Update to your own filepath
#     print(path_to_file)
#     # move to Google Drive directory
#     os.chdir(path_to_file)
#     !pwd

# Define paths
# job_post_skills_path = "linkedin_job_posts_skills.csv"   #sys.argv[1]
# stopwords_path = "stopwords.txt"  #sys.argv[2]
# job_seeker_path = "test_cases.csv"  #sys.argv[3]
# output_file_path = "output_job_skills_match.csv"  #sys.argv[4]
job_post_skills_path = sys.argv[1]
stopwords_path = sys.argv[2]
job_seeker_path = sys.argv[3]
output_file_path = sys.argv[4]

# Initialize Spark session
spark = SparkSession.builder \
    .appName("MatchJobSkills") \
    .getOrCreate()

"""## 2. Load Spark Dataframe"""

# Load posting & skills CSV into DataFrame
df_posts_skills = spark.read.csv(job_post_skills_path, header=True, inferSchema=True)

# Count number of rows
num_rows = df_posts_skills.count()
print(num_rows)

# Show the DataFrame schema and some sample data
df_posts_skills.printSchema()
df_posts_skills.show(5)  # Show the first 5 rows

df_posts_skills.tail(1)

"""## 3. Pre-process the "job_skills" column"""

# Initialize NLTK objects
lemmatizer = WordNetLemmatizer()
stopwords_nltk = list(stopwords.words("english"))

# Load stopwords from the file into a list
with open(stopwords_path, "r") as f:
    stopwords_txt = list(f.read().splitlines())

# Combine stopwords from nltk & txt
stop_words = set(stopwords_nltk + stopwords_txt)

# Function to clean job_skills
def clean_tokenize_skills(skills):
    skills = re.sub(r'[^a-zA-Z,\s]', '', skills) # Remove non-alphabetical characters except commas and spaces
    skills = re.sub(r'\s*,\s*', ',', skills) # Remove extra spaces around commas
    skills = re.sub(r'\s+', ' ', skills)  # Remove extra spaces between words
    skills = skills.lower() # Convert to lowercase
    skills_list = skills.split(',') # Split skills by comma
    cleaned_skills = []
    for skill in skills_list:
        skill = skill.strip() # Remove leading and trailing whitespace
        if skill not in stop_words: # Remove stopwords
            skill = lemmatizer.lemmatize(skill) # Lemmatize skill
            skill = skill.replace(string.punctuation, '') # within each skill (element in the list), remove all punctuation including commas (as there should be no punctuation within each element)
            cleaned_skills.append(skill)
    return cleaned_skills

# Define UDF to apply cleaning and tokenization function
clean_tokenize_udf = udf(clean_tokenize_skills, ArrayType(StringType()))

# Apply UDF to clean and tokenize job_skills column
df_posts_skills_cleaned = df_posts_skills.withColumn("cleaned_job_skills", clean_tokenize_udf(col("job_skills")))

# Show cleaned DataFrame
# df_posts_skills_cleaned.show(truncate=False)

# df_posts_skills_cleaned.tail(1)

# Show the DataFrame schema and some sample data
df_posts_skills_cleaned.printSchema()
df_posts_skills_cleaned.show(5)

# Observe "processed_job_skills" column
df_posts_skills_cleaned.select("job_skills", "cleaned_job_skills").show(5, truncate=False)

# Observe last row
df_posts_skills_cleaned.tail(1)

"""## 4. Apply CountVectorizer

Similar to Term Frequency (of each skills phrase)
"""

# Apply CountVectorizer to count the occurrences of each phrase
vectorizer = CountVectorizer(inputCol="cleaned_job_skills", outputCol="skills_frequency", minDF=100)

vectorizer_model = vectorizer.fit(df_posts_skills_cleaned)
df_posts_skills_cleaned_vectorise = vectorizer_model.transform(df_posts_skills_cleaned)

# Show the DataFrame schema and some sample data
df_posts_skills_cleaned_vectorise.printSchema()
df_posts_skills_cleaned_vectorise.show(5)

# Observe "processed_job_skills" column
df_posts_skills_cleaned_vectorise.tail(3)

"""## 5. Calculate TFIDF"""

# Compute Inverse Document Frequency (IDF)
# Create IDF instance to compute inverse document frequencies
idf = IDF(inputCol="skills_frequency", outputCol="skills_tfidf")
idf_model = idf.fit(df_posts_skills_cleaned_vectorise)

# Compute TF-IDF
# Call "fit" to compute the IDF vector and then call "transform" to scale the term frequencies by IDF.
df_posts_skills_tfidf = idf_model.transform(df_posts_skills_cleaned_vectorise)

# Show the DataFrame schema and some sample data
df_posts_skills_tfidf.printSchema()
df_posts_skills_tfidf.show(5)

# Observe "processed_job_skills" column
df_posts_skills_tfidf.tail(3)

"""## 6. Preparing the User Profile (Preprocess Skills)"""

# Load CSV into DataFrame
df_queries = spark.read.csv(job_seeker_path, header=True, inferSchema=True)

df_queries.show(truncate=False)

# Define function to clean "skills" column & transform into skills vector + calculate TF-IDF

# Apply the same UDF to clean and tokenize job_skills column
df_queries_cleaned = df_queries.withColumn("cleaned_job_skills", clean_tokenize_udf(col("job_skills")))

# Transform into vector, using the vectorizer_model fitted on the list of job postings
df_queries_vectorise = vectorizer_model.transform(df_queries_cleaned)

# Compute the TF-IDF vector by calling "transform" based on the fitted idf_model.
df_queries_tfidf = idf_model.transform(df_queries_vectorise)

df_queries_tfidf.show(truncate=False)

"""## 7. Building the SQL Query & Filtered Item Profile (Filtered Skills Matrix)"""

# Register DataFrame as a temporary view (to facilitate querying with SQL)
df_posts_skills_tfidf.createOrReplaceTempView("df_posts_skills_tfidf_view")

# query_trial = """
# SELECT *
# FROM df_posts_skills_tfidf_view
# WHERE job_title LIKE '%Data Scientist%'
# AND search_city = 'Seattle';
# """

# spark.sql(query_trial).show(truncate=False)

query_generator(search_city="Seattle", search_country="United States", job_level="Mid senior", job_type="Onsite")   #            |United States |Mid senior|Onsite

# Define function to build SQL query
def query_generator(search_city=None, search_country=None, job_level=None, job_type=None):
    """
    Generate a dynamic SQL query based on user inputs and execute it on the DataFrame.

    Args:
        df (DataFrame): The PySpark DataFrame to query.
        search_city (str, optional): The search city. Defaults to None.
        search_country (str, optional): The search country. Defaults to None.
        job_level (str, optional): The job level. Defaults to None.
        job_type (str, optional): The job type. Defaults to None.

    Returns:
        DataFrame: The resulting DataFrame after applying the query.
    """

    # Initialize an empty list to store conditions
    conditions = []

    # Add conditions based on user inputs
    if search_city:
        conditions.append(f"search_city = '{search_city}'")

    if search_country:
        conditions.append(f"search_country = '{search_country}'")

    if job_level:
        conditions.append(f"job_level = '{job_level}'")

    if job_type:
        conditions.append(f"job_type = '{job_type}'")

    # Construct the WHERE clause by joining the conditions
    where_clause = " AND ".join(conditions)

    # Write the SQL query with the dynamic WHERE clause
    query = f"SELECT * FROM df_posts_skills_tfidf_view WHERE {where_clause}"

    return query

# Testing Queries

print(query_generator(search_city='Atlanta', search_country='United States', job_level='Associate', job_type='Onsite'))

print(query_generator(search_city='Atlanta', job_level='Associate', job_type='Hybrid'))

print(query_generator(search_country='United Kingdom', job_level='Mid senior', job_type='Remote'))

print(query_generator(search_country='Canada', job_type='Remote'))

print(query_generator(search_country='Australia'))

print(query_generator(search_country='Australia', job_level=None, job_type=None))

"""## 8. Cosine Similarity Function"""

# Define a user-defined function (UDF) to calculate cosine similarity between two SparseVectors
# Formula: cos(a,b) = (a.b) / (||a||*||b||)

def cosine_similarity(v1, v2):
    """
    Calculate cosine similarity between two SparseVectors.
    """
    dot_product = float(v1.dot(v2))   # Calculate dot product of SparseVector v1 & v2
    norm_v1 = float(v1.norm(2))   # Find norm of the SparseVector v1
    norm_v2 = float(v2.norm(2))   # Find norm of the SparseVector v2

    # Handle Zero Division cases   # Return 0 for similarity when either of the vectors is a zero vector
    if norm_v1 * norm_v2 == 0:
        return 0.0

    similarity = dot_product / (norm_v1 * norm_v2)   # Calculate cosine similarity
    return similarity

# Wrap the function around a UDF (required to perform operations in Spark dataframes)
cosine_similarity_udf = udf(cosine_similarity, DoubleType())

"""## 9. Matching Most Relevant Jobs Based on User's Skills"""

# Create a list to hold the results
query_skills_best_jobs = []

# FOR EACH QUERY (row):   # Iterate through each row of the user profile DataFrame
for row_query in df_queries_tfidf.collect():

    # Select each candidate (row in df_queries_tfidf) & get the necessary information  # select only
    # row_query = df_queries_tfidf.head(1)[0]  #### Need to modify accordingly later
    print(row_query)

    row_sn = row_query['sn']
    row_search_city = row_query['search_city']   # <class 'str'>
    row_search_country = row_query['search_country']
    row_job_level = row_query['job_level']
    row_job_type = row_query['job_type']
    row_job_skills = row_query['job_skills']
    row_skills_tfidf = row_query['skills_tfidf']  # <class 'pyspark.ml.linalg.SparseVector'>

    # Construct SQL search query: search_city|search_country|job_level|job_type|job_skills
    row_sql_query = query_generator(search_city=row_search_city, search_country=row_search_country, job_level=row_job_level, job_type=row_job_type)

    # Filter the relevant rows (run SQL query)
    temp_relevant_posts_skills_df = spark.sql(row_sql_query)

    # Define a UDF function to wrap around the "row_skills_tfidf" SparseVector.
    # This was done in preparation for the next step to calculate cosine similarity, as I could not use the SparseVector directly for calculation.
    def row_skills_tfidf_vector():
        return row_skills_tfidf
    row_skills_tfidf_vector = udf(row_skills_tfidf_vector, VectorUDT())

    # Compute similarity score using the UDF for each relevant row
    temp_relevant_posts_skills_df = temp_relevant_posts_skills_df.withColumn("similarity_score", cosine_similarity_udf(col("skills_tfidf"), row_skills_tfidf_vector()))

    # Sort in descending order based on similarity_score
    temp_relevant_posts_skills_df = temp_relevant_posts_skills_df.orderBy(col("similarity_score").desc())

    # Identify top 5 best matches
    # Extract the top 5 jobs along with the corresponding columns into a Python list of dictionaries
    top5_relevant_jobs = temp_relevant_posts_skills_df.select("job_link_id", "job_title", "company", "job_location", "job_level", "job_type", "job_skills", "similarity_score").limit(5).collect()

    # Add tags and ranks to each job dictionary
    for rank, job in enumerate(top5_relevant_jobs, start=1):
        job_dict = {"query_sn": row_sn, "users_skills": row_job_skills, "job_suitability_rank": rank}  # Create new dictionary with tag and rank
        job_dict.update(job.asDict())  # Update with original dictionary
        query_skills_best_jobs.append(job_dict)

    print("Row complete")

# Extract keys from the first dictionary in the list to use as column names
column_names = query_skills_best_jobs[0].keys()

# Write the data to the CSV file
with open(output_file_path, 'w', newline='') as csvfile:
    writer = csv.DictWriter(csvfile, fieldnames=column_names)

    # Write column names to the CSV file
    writer.writeheader()

    # Write each row of data to the CSV file
    for job in query_skills_best_jobs:
        writer.writerow(job)

print(f"Data has been successfully written to {output_file_path}")